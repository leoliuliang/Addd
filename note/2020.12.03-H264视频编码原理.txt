视频文件封装格式：
也叫容器，就是将已经编码压缩好的视频轨和音频轨按照一定的格式放到一个文件中。可以有多个视频（如画中画）、多个音频（如混音）。

ffmpeg将视频数据抽取出来为out.h264：
ffmpeg -i input.mp4 -c:v copy -bsf:v h264_mp4toannexb -an out.h264

播放h264文件：
ffplay -stats -f h264 out.h264

ffmpeg将视频文件的音频提取出来命名为output.aac：
ffmpeg -i input.mp4 -acodec copy -vn output.aac

视频编码的作用：将 视频像素数据（RGB，YUV等）压缩成视频码流，从而降低视频的数据量。
音频编码的作用：将音频采样数据（PCM等）压缩成音频码流，从而降低音频的数据量。

什么是H264：
对摄像头采集的每一帧视频需要进行编码，由于视频中存在空间和时间的冗余，需要用算法来去除这些冗余。H264是专门去除这些冗余的算法，我们把这种算法称为H264编码。
H264是新一代编码标准，以高压缩高质量和支持多种网络的流媒体传输著称。

无论是h264、mpeg4、vp9都是基于宏块的方式进行编码，原理是一样的，只不过实现的算法不一样罢了。

H261音视频编码鼻祖奠定的技术：
·帧内编码：第一次根据帧内的像素趋于统一，而采用帧内预测编码技术。
·帧间编码（运动补偿）：使用以宏块为基础的运动补偿预测编码技术，从当前宏块参考帧中产生最佳匹配宏块。
·环路滤波器：实际上是一个数字低筒滤波器，滤除不必要的高频信息。
·块结构的混合编码：第一种采用”块结构的混合编码“方案的编码标准。

摄像头捕捉的数据是yuv数据

编码过程：
摄像头->信源编码器->视频复合编码器->传输缓冲器->传输编码器->h264码流
信源编码器：可将一帧数据切分为很多宏块，宏块大小为16*16，还可以划分为更小的子块（4*4，8*4，4*8，8*8，8*16，16*8）。
视频复合编码器：将宏块数据、左边上边像素、预测方向、配置信息等整合。 
传输缓冲器：将比较小的宏块先缓存起来（比如B帧），等到凑够数后（比如I帧来了，凑够一个完整帧）一起输出。
传输编码器：编码后输出。

块预测还原图像：因为颜色相近称为空间冗余，所以h264算法是只记录上边和左边的像素，再加上预测方向算法就可以还原图像，比如一个8*8的宏块就只需要（8+8）-1=15字节，减1是因为左上角重合，比存64字节少了很多。

帧内压缩：宏块只记录上边左边像素和预测信息，16*16的宏块还可以划分更小的子宏块，比如4*4、8*16等。
帧间压缩：一个宏块第二帧发生位置移动，那就只会计算坐标信息，不再对左边上边数据计算，左边上边数据可以取缓存。

视频播放的本质就是宏块移动（宏块坐标位置发生改变）

H264编码：与I帧相似程度极高到达95%以上，编码成B帧，相似程度70%编码成P帧，如何编码不需要程序员来实现，已经由x264这个工具做了。

编码顺序：I帧->P帧->B帧->B帧

H264数据流：
除了I/P/B帧外，还有图像序列GOP，GOP图像序列可以理解成一个场景，场景的物体都是相似的。
GOP：在两个I帧之间是一个图像序列，在一个图像序列中只有一个I帧；所以在短视频里gop越长越好，越长I帧就越少，数据就越小；直播里gop越短越好，因为I帧是携带的完整数据，B/P帧只是携带坐标、配置、矢量等数据，不能被完整解码，所以需要I帧出现的频率高用户才不会等待视频卡顿，为了解决数据量大，直播的帧率都比较低，一般10帧/秒。


